data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# CART methodology classification
# Import necessary library for CART methodology and evaluation
library(rpart)
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Build classification tree model using CART algorithm
CART_classification <- rpart(Abs_cat~., data = train_data)
rpart.plot(CART_classification)
# Predict target class using predict function
predictions <- predict(CART_classification, test_data)
View(predictions)
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# CART methodology classification
# Import necessary library for CART methodology and evaluation
library(rpart)
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Build classification tree model using CART algorithm
CART_classification <- rpart(Abs_cat~., data = train_data, method="class")
rpart.plot(CART_classification)
# Predict target class using predict function
predictions <- predict(CART_classification, test_data, type="class")
View(predictions)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
print(confusion_matrix)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test$Abs_cat != "Abs_High")
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", confusion_matrix$byClass['Precision']))
cat(sprintf("Precision: %f\n", precision))
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# CART methodology classification
# Import necessary library for CART methodology and evaluation
library(rpart)
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Build classification tree model using CART algorithm
CART_classification <- rpart(Abs_cat~., data = train_data, method="class")
rpart.plot(CART_classification)
# Predict target class using predict function
predictions <- predict(CART_classification, test_data, type="class")
View(predictions)
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
print(confusion_matrix)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# Random Forest methodology classification
# Import necessary library for Random Forest methodology and evaluation
library(randomForest)
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Fit classification tree model using Random Forest algorithm
random_forest_classification <- randomForest(Abs_cat~., data = train_data, importance = TRUE, ntree = 500)
importance(random_forest_classification)
varImpPlot(random_forest_classification)
importance(random_forest_classification)
# Random Forest methodology classification
# Import necessary library for Random Forest methodology and evaluation
library(randomForest)
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Fit classification tree model using Random Forest algorithm
random_forest_classification <- randomForest(Abs_cat~., data = train_data, importance = TRUE, ntree = 500)
importance(random_forest_classification)
varImpPlot(random_forest_classification)
print(random_forest_classification)
# Random Forest methodology classification
# Import necessary library for Random Forest methodology and evaluation
library(randomForest)
importance(random_forest_classification)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# Random Forest methodology classification
# Import necessary library for Random Forest methodology and evaluation
library(randomForest)
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Fit classification tree model using Random Forest algorithm
random_forest_classification <- randomForest(Abs_cat~., data = train_data, importance = TRUE, ntree = 500)
importance(random_forest_classification)
randomForest::importance(random_forest_classification)
varImpPlot(random_forest_classification)
print(random_forest_classification)
# Predict target class using predict function
predictions <- predict(random_forest_classification, test_data, type = "class")
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# Random Forest methodology classification
# Import necessary library for Random Forest methodology and evaluation
library(randomForest)
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Fit classification tree model using Random Forest algorithm
random_forest_classification <- randomForest(Abs_cat~., data = train_data, importance = TRUE, ntree = 500)
randomForest::importance(random_forest_classification)
varImpPlot(random_forest_classification)
print(random_forest_classification)
# Predict target class using predict function
predictions <- predict(random_forest_classification, test_data, type = "class")
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
print(confusion_matrix)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# C5.0 methodology classification
# Import necessary library for C5.0 methodology and evaluation
library('C50')
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Fit classification tree model using C5.0 algorithm
C50_classification <- C5.0(Abs_cat~., data = train_data)
summary(C50_classification)
plot(C50_classification)
# Predict target class using predict function
predictions <- predict(C50_classification, test_data, type = "class")
warnings()
# Predict target class using predict function
predictions <- predict(C50_classification, test_data, type = "class")
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
print(confusion_matrix)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# CART methodology classification
# Import necessary library for CART methodology and evaluation
library(rpart)
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Build classification tree model using CART algorithm
CART_classification <- rpart(Abs_cat~., data = train_data, method="class")
rpart.plot(CART_classification)
# Predict target class using predict function
predictions <- predict(CART_classification, test_data, type="class")
View(predictions)
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
print(confusion_matrix)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Split data into 2 parts: 70% as training data, and 30% as testing data
split_data <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[split_data, ]
test_data <- data[-split_data, ]
# CART methodology classification
# Import necessary library for CART methodology and evaluation
library(rpart)
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
library(class)
library(caret)
# Set the seed to reproduce a particular sequence of 'random' numbers
set.seed(123)
# Build classification tree model using CART algorithm
CART_classification <- rpart(Abs_cat~., data = train_data, method="class")
rpart.plot(CART_classification)
# Predict target class using predict function
predictions <- predict(CART_classification, test_data, type="class")
View(predictions)
# Create a confusion matrix to compare the prediction to original data
confusion_matrix <- confusionMatrix(data = predictions, reference = test_data$Abs_cat)
cat(sprintf("Accuracy: %f\n", confusion_matrix$overall['Accuracy']))
tp <- sum(predictions == "Abs_High" & test_data$Abs_cat == "Abs_High")
fp <- sum(predictions == "Abs_High" & test_data$Abs_cat != "Abs_High")
precision <- tp / (tp + fp)
cat(sprintf("Precision: %f\n", precision))
print(confusion_matrix)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE, na.strings = '?')
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 3:ncol(data)])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
dev.off()
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
dev.off()
# Cuts a tree from "hclust_results" into 2 groups
hclust_2 <- cutree(hclust_results, 2)
table(h_cluster=hclust_2, data[, 2])
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
dev.off()
# Cuts a tree from "hclust_results" into 3 groups
hclust_3 <- cutree(hclust_results, k=3)
table(h_cluster=hclust_3, data[, 4])
# K-Means
# Perform k-means clustering on a data matrix with 3 clusters
kmeans_3 <- kmeans(data[, 1:3], centers=3)
kmeans_3$cluster
kmeans_3$centers
table(k_means_cluster=kmeans_3$cluster, data[, 4])
# Plot K-means clusters vs Abs_cat
plot_cluster(kmeans_3, data = data, geom = "point",
stand = FALSE, ellipse.type = "t",
ellipse.level = 0.95, ggtheme = theme_minimal()) +
ggtitle("K-means Clusters vs. Abs_cat")
library(cluster)
library(factoextra)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
# Cuts a tree from "hclust_results" into 3 groups
hclust_3 <- cutree(hclust_results, k=3)
table(h_cluster=hclust_3, data[, 4])
# K-Means
# Perform k-means clustering on a data matrix with 3 clusters
kmeans_3 <- kmeans(data[, 1:3], centers=3)
kmeans_3$cluster
kmeans_3$centers
table(k_means_cluster=kmeans_3$cluster, data[, 4])
View(kmeans_3)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
dev.off()
# Cuts a tree from "hclust_results" into 3 groups
hclust_3 <- cutree(hclust_results, k=3)
table(h_cluster=hclust_3, data[, 4])
# K-Means
# Perform k-means clustering on a data matrix with 3 clusters
kmeans_3 <- kmeans(data[, 1:3], centers=3)
kmeans_3$cluster
kmeans_3$centers
plot(hclust_results)
rm(list=ls())
# Upload data set
name <- file.choose()
data <- read.csv(name, header = TRUE)
View(data)
# Identify missing values
missing_values <- is.na(data)
missing_values
# Remove the rows with missing values
data <- na.omit(data)
# Define the outcome variable as a factor
data$Abs_cat <- as.factor(data$Abs_cat)
# Hierarchical clustering
# Computes and returns the distance matrix (Exclude column 1 and 2)
data_dist <- dist(data[, 1:3])
# Perform Hierarchical clustering
hclust_results <- hclust(data_dist)
plot(hclust_results)
# Cuts a tree from "hclust_results" into 3 groups
hclust_3 <- cutree(hclust_results, k=3)
table(h_cluster=hclust_3, data[, 4])
# K-Means
# Perform k-means clustering on a data matrix with 3 clusters
kmeans_3 <- kmeans(data[, 1:3], centers=3)
kmeans_3$cluster
kmeans_3$centers
table(k_means_cluster=kmeans_3$cluster, data[, 4])
